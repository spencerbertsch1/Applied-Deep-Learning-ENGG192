{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# XOR Analysis\n",
        "\n",
        "### XOR analysis using dense, filly connected neural netowrks\n",
        "\n",
        "ENGG 192 - Dartmouth COllege <br>\n",
        "Winter 2019 <br>\n",
        "Spencer Bertsch\n",
        "\n",
        "Many of the ideas for this code were lifted directly from the sources including \"Deep Learning\" by by Ian Goodfellow and Yoshua Bengio and Aaron Courville. The point here was not to solve a difficult, existing problem, but rather to develop an intuation for deep, feedforward neural networks by solving an intentionally trivial problem which requires a small amount of training data. In \"Deep Learning\", they discuss the importance of deploying a very simple feed forward network beginning with one layer and eventually groeing to two total layers (one hidden layer). By performing this exercise, engineers can understand how a single function such as logistic regression - which can be thought of as a single neuron - can be built into a system of neurons to create a learning tool which can more effectively learn representations of complex functions.  \n",
        "\n",
        "**Sources** <br> \n",
        "The main source for this notebook is the textbook \"Deep Learning\" by Ian Goodfellow and Yoshua Bengio and Aaron Courville. \n",
        "\n",
        "@book{Goodfellow-et-al-2016, <br>\n",
        "    title={Deep Learning}, <br>\n",
        "    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville}, <br>\n",
        "    publisher={MIT Press}, <br>\n",
        "    note={\\url{http://www.deeplearningbook.org}}, <br>\n",
        "    year={2016} <br>\n",
        "}\n",
        "\n",
        "Other sources include: <br> \n",
        "https://gist.github.com/RichardKelley/17ef5f2291c273de11540c33dc1bfbf2 <br> \n",
        "https://github.com/hunkim/PyTorchZeroToAll <br> \n",
        "https://github.com/hunkim/PyTorchZeroToAll/blob/master/06_logistic_regression.py <br> \n",
        "\n",
        "The above three sources have been extremely helpful when familiarizing myself with pytorch and the XOR problem. \n",
        "\n",
        "https://pytorch.org/ <br> \n",
        "https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 <br>\n",
        "https://www.youtube.com/watch?v=GAKTBQn7yKo&t=516s <br>\n",
        "https://www.youtube.com/watch?v=113b7O3mabY <br> \n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable #<-- Variable can be thought of as a Torch matrix\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XOR Function\n",
        "\n",
        "Source - \"Deep Learning\", Goodfellow, Bengio and Courville - Chapter 6.1\n",
        "\n",
        "The XOR (exlusive or) function takes two values (x1 and x2) as its inputs and provides binary output. The function itself is very simple; if one of the inputs (x1 OR x2) are equal to 1, then the XOR function returns a one, otherwise the function returns a zero. \n",
        "\nOur goal here will be to develop a feedforward neural network with the function y = f(x;θ) and train the network on examples so that it can learn the parameters in θ. The end goal will be to find the correct values in θ so that our function y = f(x;θ) can closely approximate y = f*(x), the true XOR function. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the neural network dedicated to learning the XOR function f*(x)\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class XorNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \"super can be used to refer to parent classes without naming them explicitly\"\n",
        "        docs.python.org/2/library/functions.html#super\n",
        "        \n",
        "        Here we construct a neural network model\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        #add one hidden layer with three neurons\n",
        "        self.fc1 = nn.Linear(2,3)\n",
        "        #add one hidden layer with three neurons\n",
        "        self.fc2 = nn.Linear(3,1) \n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Our forward function takes in the input data and uses the relu activation function to generate the output\n",
        "        \"\"\"\n",
        "        x = F.relu(self.fc1(x)) #<-- from Torch.nn we get 'functional' and we use the relu activation\n",
        "        y_pred = self.fc2(x)\n",
        "        return y_pred"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define our model and several hyperparameters including the loss function, optomizer, epochs, and minibatch size. Next week we will go over regularization techniques to optimize these parameters to create the model which best approximates the function f*(x). "
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = XorNet() #<-- Use the model created above \n",
        "#We use mean squared error as our loss function becasue the problem at hand is very simple \n",
        "loss_fn = nn.MSELoss()\n",
        "#Adam is a very strong genric optimizer, so we can use it here\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "training_epochs = 3000\n",
        "minibatch_size = 32"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now generate our training data consisting of the set {(0,0), (1,0), (0,1), (1,1)}, and concatenate them to create our X train and y train datasets. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# input-output pairs\n",
        "pairs = [(np.asarray([0.0,0.0]), [0.0]),\n",
        "         (np.asarray([0.0,1.0]), [1.0]),\n",
        "         (np.asarray([1.0,0.0]), [1.0]),\n",
        "         (np.asarray([1.0,1.0]), [0.0])]\n",
        "\n",
        "#np.vstack stacks vectors row-wise, similar to vertcat in matlab \n",
        "X_train = np.vstack([x[0] for x in pairs]) \n",
        "y_train = np.vstack([x[1] for x in pairs])"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can finally train our model. We first predict our data using (model) to generate y_pred, then we calculate the loss (or error) for that neuron. Becasue this is a simple problem, we can just use the mean squared error function for loss. \n",
        "\n",
        "Remember that loss functions are simply the way that the network calculates the error for each neuron and then the optimizer can update the weights (W,b) which define θ and are the key learned parameters in the network. \n",
        "\n\n",
        "$J(θ) = (1/4)SUM(f*(x) - f(x;θ))^2$\n",
        "\n",
        "Where $J(θ)$ is simply the error calculated at that point. Another popular loss function used for calculating the error between the predicted and the true value inside the netowrk is the mean absolute error function. \n",
        "\n",
        "$J(θ) = (1/4)SUM|f*(x) - f(x;θ)|$\n",
        "\n",
        "This function would also work well for this application; the only difference being that the difference between the true function $f(x)$ and the approximated function $f*(x;θ)$ is taken as a positive real instead of being squared. \n",
        "\n\nThere are many other loss functions which can be used to achieve specific goals. Binary crossentropy, for example, is often used for binary classification problems. "
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(training_epochs):\n",
        "        \n",
        "    for batch_ind in range(4):\n",
        "        # wrap the data in variables\n",
        "        minibatch_state_var = Variable(torch.Tensor(X_train))\n",
        "        minibatch_label_var = Variable(torch.Tensor(y_train))\n",
        "                \n",
        "        # forward pass\n",
        "        y_pred = model(minibatch_state_var)\n",
        "        \n",
        "        # loss is MSE - defined above \n",
        "        # compute loss as the difference between the prediction and the true label \n",
        "        loss = loss_fn(y_pred, minibatch_label_var)\n",
        "\n",
        "        # now that the forward pass is done, we need to reset all gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # backwards pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # step the optimizer - update the weights\n",
        "        optimizer.step()"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now test the model by providing each of the four possible function inputs and observing the model predictions. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Test the model with each of our four inputs: \n",
        "input_1 = Variable(torch.Tensor([0,0])) # \"variable\" simply creates a new tenosr\n",
        "input_2 = Variable(torch.Tensor([1,0])) # \"variable\" simply creates a new tenosr\n",
        "input_3 = Variable(torch.Tensor([0,1])) # \"variable\" simply creates a new tenosr\n",
        "input_4 = Variable(torch.Tensor([1,1])) # \"variable\" simply creates a new tenosr\n",
        "        \n",
        "#Make predictions on the inputs \n",
        "pred_1 = model(input_1)\n",
        "pred_2 = model(input_2)\n",
        "pred_3 = model(input_3)\n",
        "pred_4 = model(input_4)\n",
        "\n",
        "#And finally print the results of prediction\n",
        "print(\"Prediction for the 1st inputs f(x1=0, x2=0)\", pred_1)\n",
        "print(\"Prediction for the 2nd inputs f(x1=1, x2=0)\", pred_2)\n",
        "print(\"Prediction for the 3rd inputs f(x1=0, x2=1)\", pred_3)\n",
        "print(\"Prediction for the 4th inputs f(x1=1, x2=1)\", pred_4)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for the 1st inputs f(x1=0, x2=0) tensor([2.9917e-30], grad_fn=<AddBackward0>)\n",
            "Prediction for the 2nd inputs f(x1=1, x2=0) tensor([1.], grad_fn=<AddBackward0>)\n",
            "Prediction for the 3rd inputs f(x1=0, x2=1) tensor([1.], grad_fn=<AddBackward0>)\n",
            "Prediction for the 4th inputs f(x1=1, x2=1) tensor([2.9917e-30], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "execution_count": 30,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the model performed very well. Although our network only has one hidden layer with three neurons, it was able to learn the correct values in θ which allowed it to approximate f(x,θ) to f*(x). Note that the model predicted ~2.99(10^-30) which is very close to zero for f(0,0) and f(1,1). "
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}