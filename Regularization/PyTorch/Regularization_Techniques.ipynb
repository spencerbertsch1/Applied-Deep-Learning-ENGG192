{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization in Deep Learning\n",
        "\n",
        "-------\n",
        "\n",
        "ENGG 192 <br>\n",
        "Dartmouth College - January 25, 2019 <br>\n",
        "Spencer Bertsch\n",
        "\n",
        "In this notebook I will discuss several techniques for regularization and ways of improving deep learning models. \n",
        "\n",
        "I discuss how random search is a better method for hyperparameter optimization than manual or grid search. I also discuss other methods of regularization such as L1 and L2 regularization, early stopping, and dropout layers. \n",
        "\n",
        "I end by discussing methods for automated hyperparameter tuning (i.e. AutoKeras)\n",
        "\n",
        "In addition to discussing the differences between these techniques and the specific use cases for each, I will apply some of them on a real world example to see how they work on an example dataset. I found a fun dataset for this notebook: the data provides a numerical description of many different types of mushrooms and the binary label vector shows whether or not the mushroom is poisonous. \n",
        "\n",
        "The goal of the model is to approximate f*(x), a model which can use a muchroom's growing location and other characteristics to determine whether or not it's poisonous. I chose this dataset for several reasons, one being its size. Becasue it's relatively small, it will lend itself to quick training and testing. \n",
        "\n",
        "The data source for this notebook is the [Mushroom Classification](https://www.kaggle.com/uciml/mushroom-classification) which is held by Kaggle. \n",
        "\n",
        "Other sources for this notebook are discussed in the \"Deep Learning\" sections below, but some of the larger sources include: \n",
        "\n",
        "[Sung Kim's Github](https://github.com/hunkim) <br>\n",
        "[Stanford PyTorch Github](https://cs230-stanford.github.io/pytorch-getting-started.html)\n",
        "\n",
        "\"Deep Learning\" by Ian Goodfellow and Yoshua Bengio and Aaron Courville. \n",
        "\n",
        "@book{Goodfellow-et-al-2016, <br>\n",
        "    title={Deep Learning}, <br>\n",
        "    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville}, <br>\n",
        "    publisher={MIT Press}, <br>\n",
        "    note={\\url{http://www.deeplearningbook.org}}, <br>\n",
        "    year={2016} <br>\n",
        "}\n",
        "\n\n"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep learning \n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable #<-- Variable can be thought of as a Torch matrix\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import math\n",
        "\n",
        "#Data cleaning and preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Model evaluation\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score"
      ],
      "outputs": [],
      "execution_count": 368,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Dataset \n",
        "\nOur dataset exists as a .csv file downloaded form Kaggle. In a more developed project we could implement a more efficient way for data to be loaded into the notebook, but for now we can simply use a handy feature of Pandas called 'read_csv' to import a .csv to a pandas dataframe. "
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the dataset - load .csv file\n",
        "path = '/Users/spencerbertsch/Desktop/ENGG 192/Regularization/Data/mushrooms.csv'\n",
        "#create a dataframe from the Audit data stored in the .csv file\n",
        "data_raw = pd.read_csv(path)\n",
        "\n",
        "print(\"Shapw of raw dataset:\", (data_raw.shape))\n",
        "print(\"So we can see that we have\" , (data_raw.shape[1] - 1) , \"features and one label vector\")\n",
        "print(\"We can also see that we have\", data_raw.shape[0] , \"rows\")\n",
        "print(\" \")\n",
        "print(\"We can also preview our dataset by observing the top five rows\")\n",
        "data_raw.head(5)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapw of raw dataset: (8124, 23)\n",
            "So we can see that we have 22 features and one label vector\n",
            "We can also see that we have 8124 rows\n",
            " \n",
            "We can also preview our dataset by observing the top five rows\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 369,
          "data": {
            "text/plain": [
              "  class cap-shape cap-surface cap-color bruises odor gill-attachment  \\\n",
              "0     p         x           s         n       t    p               f   \n",
              "1     e         x           s         y       t    a               f   \n",
              "2     e         b           s         w       t    l               f   \n",
              "3     p         x           y         w       t    p               f   \n",
              "4     e         x           s         g       f    n               f   \n",
              "\n",
              "  gill-spacing gill-size gill-color   ...   stalk-surface-below-ring  \\\n",
              "0            c         n          k   ...                          s   \n",
              "1            c         b          k   ...                          s   \n",
              "2            c         b          n   ...                          s   \n",
              "3            c         n          n   ...                          s   \n",
              "4            w         b          k   ...                          s   \n",
              "\n",
              "  stalk-color-above-ring stalk-color-below-ring veil-type veil-color  \\\n",
              "0                      w                      w         p          w   \n",
              "1                      w                      w         p          w   \n",
              "2                      w                      w         p          w   \n",
              "3                      w                      w         p          w   \n",
              "4                      w                      w         p          w   \n",
              "\n",
              "  ring-number ring-type spore-print-color population habitat  \n",
              "0           o         p                 k          s       u  \n",
              "1           o         p                 n          n       g  \n",
              "2           o         p                 n          n       m  \n",
              "3           o         p                 k          s       u  \n",
              "4           o         e                 n          a       g  \n",
              "\n[5 rows x 23 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>cap-shape</th>\n",
              "      <th>cap-surface</th>\n",
              "      <th>cap-color</th>\n",
              "      <th>bruises</th>\n",
              "      <th>odor</th>\n",
              "      <th>gill-attachment</th>\n",
              "      <th>gill-spacing</th>\n",
              "      <th>gill-size</th>\n",
              "      <th>gill-color</th>\n",
              "      <th>...</th>\n",
              "      <th>stalk-surface-below-ring</th>\n",
              "      <th>stalk-color-above-ring</th>\n",
              "      <th>stalk-color-below-ring</th>\n",
              "      <th>veil-type</th>\n",
              "      <th>veil-color</th>\n",
              "      <th>ring-number</th>\n",
              "      <th>ring-type</th>\n",
              "      <th>spore-print-color</th>\n",
              "      <th>population</th>\n",
              "      <th>habitat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>p</td>\n",
              "      <td>x</td>\n",
              "      <td>s</td>\n",
              "      <td>n</td>\n",
              "      <td>t</td>\n",
              "      <td>p</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>n</td>\n",
              "      <td>k</td>\n",
              "      <td>...</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>p</td>\n",
              "      <td>k</td>\n",
              "      <td>s</td>\n",
              "      <td>u</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>e</td>\n",
              "      <td>x</td>\n",
              "      <td>s</td>\n",
              "      <td>y</td>\n",
              "      <td>t</td>\n",
              "      <td>a</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "      <td>k</td>\n",
              "      <td>...</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>p</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>e</td>\n",
              "      <td>b</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>t</td>\n",
              "      <td>l</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "      <td>n</td>\n",
              "      <td>...</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>p</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>p</td>\n",
              "      <td>x</td>\n",
              "      <td>y</td>\n",
              "      <td>w</td>\n",
              "      <td>t</td>\n",
              "      <td>p</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>...</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>p</td>\n",
              "      <td>k</td>\n",
              "      <td>s</td>\n",
              "      <td>u</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>e</td>\n",
              "      <td>x</td>\n",
              "      <td>s</td>\n",
              "      <td>g</td>\n",
              "      <td>f</td>\n",
              "      <td>n</td>\n",
              "      <td>f</td>\n",
              "      <td>w</td>\n",
              "      <td>b</td>\n",
              "      <td>k</td>\n",
              "      <td>...</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>e</td>\n",
              "      <td>n</td>\n",
              "      <td>a</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 23 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 369,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning \n",
        "\nOur data is currently in the form of letters.. Let's clean this up using integer encoding. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#define a function to label encode a 2D (x,y) matrix and return an encoded matrix\n",
        "def int_encode(df):\n",
        "    cols = df.shape[1]\n",
        "    rows = df.shape[0]\n",
        "    original_size = df.shape[1]\n",
        "    \n",
        "    for i in range(0, cols, 1):\n",
        "        values = df.iloc[:,i]\n",
        "        #integer encode each column\n",
        "        label_encoder = LabelEncoder()\n",
        "        integer_encoded = label_encoder.fit_transform(values) #integer encode the i'th column \n",
        "        encoded_vec = pd.DataFrame(integer_encoded) #convert to pd.dataframe\n",
        "        df = pd.concat([df, encoded_vec], axis=1) #concatenate the new, encoded vector onto the original dataframe\n",
        "    \n",
        "    final_df = df.iloc[:,-cols:] #Only keep the last (leftmost) half of the columns representing the encoded data\n",
        "    print(\"Integer encoding successful!\")\n",
        "    print(\"Dataframe shape: \", final_df.shape)\n",
        "    return(final_df)"
      ],
      "outputs": [],
      "execution_count": 370,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = int_encode(data_raw)\n",
        "df.head(5) #preview our new dataset"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integer encoding successful!\n",
            "Dataframe shape:  (8124, 23)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 371,
          "data": {
            "text/plain": [
              "   0  0  0  0  0  0  0  0  0  0 ...  0  0  0  0  0  0  0  0  0  0\n",
              "0  1  5  2  4  1  6  1  0  1  4 ...  2  7  7  0  2  1  4  2  3  5\n",
              "1  0  5  2  9  1  0  1  0  0  4 ...  2  7  7  0  2  1  4  3  2  1\n",
              "2  0  0  2  8  1  3  1  0  0  5 ...  2  7  7  0  2  1  4  3  2  3\n",
              "3  1  5  3  8  1  6  1  0  1  5 ...  2  7  7  0  2  1  4  2  3  5\n",
              "4  0  5  2  3  0  5  1  1  0  4 ...  2  7  7  0  2  1  0  3  0  1\n",
              "\n[5 rows x 23 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>...</th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 23 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 371,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling\n",
        "\nWe can now scale our dataset. Becasue our data is already numerical, there's no need for any type of encoding. This means we can jump right in and use a handy Sklearn scaling package to scale our training data. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Scaling #<-- Comment out for now \n",
        "# df_scaled = preprocessing.scale(df2)\n",
        "# print(df_scaled[:,5:]) #print first 5 rows of the new numpy matrix"
      ],
      "outputs": [],
      "execution_count": 372,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split our dataset\n",
        "\nAnd with that, we're ready to split our data into a training and testing set. It would also be helpful to hold out a piece of the data for validation, but becasue we're just focusing on regularization processes, we don't need a validation section. "
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Seperate training and testing data\n",
        "y = df.iloc[:, 0] #labels are the first column in the matrix\n",
        "X = df.iloc[:, -22:] #assign feature set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "print(\"X train shape: \", X_train.shape)\n",
        "print(\"y train shape: \", y_train.shape)\n",
        "print(\"X test shape: \", X_test.shape)\n",
        "print(\"y test shape: \", y_test.shape) \n",
        "\n",
        "#Later when we convert to torch tensors, we will need our data to exist as numpy arrays, not pandas arrays \n",
        "X_train = X_train.values\n",
        "X_test = X_test.values\n",
        "y_train = y_train.values \n",
        "y_test = y_test.values"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X train shape:  (5686, 22)\n",
            "y train shape:  (5686,)\n",
            "X test shape:  (2438, 22)\n",
            "y test shape:  (2438,)\n"
          ]
        }
      ],
      "execution_count": 373,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras - TF Backend\n",
        "\n",
        "------\n",
        "\nIn addition to using PyTorch (which I prefer becasue it gives you more control over feedofrward networks than using the Keras API on a ;ibrary such as TensorFlow or Theano), I will use Keras with a TensorFlow backend simply as a benchmark. Although there's less control using Keras and Tensorflow, it's very easy to set up a quick feedforward neural network using the "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional imports \n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn import metrics"
      ],
      "outputs": [],
      "execution_count": 374,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can simply generate a 'sequential' model in Keras which will provide us with a simple, dense, fully connected neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', input_dim=22))\n",
        "model.add(Dense(units = 10, activation='relu'))\n",
        "model.add(Dense(units = 10, activation='relu'))\n",
        "model.add(Dense(units = 10, activation='relu'))\n",
        "model.add(Dense(units = 10, activation='relu'))\n",
        "model.add(Dense(units = 1, activation='sigmoid')) \n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "outputs": [],
      "execution_count": 375,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=100)#<-- Given the size of our dataset, we could definitely reduce batch size "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "5686/5686 [==============================] - 1s 210us/step - loss: 0.7308 - acc: 0.4993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/10\n",
            "5686/5686 [==============================] - 0s 21us/step - loss: 0.5840 - acc: 0.6885\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/10\n",
            "5686/5686 [==============================] - 0s 21us/step - loss: 0.4327 - acc: 0.8240\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/10\n",
            "5686/5686 [==============================] - 0s 23us/step - loss: 0.3048 - acc: 0.8845\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 5/10\n",
            "5686/5686 [==============================] - 0s 20us/step - loss: 0.2569 - acc: 0.9091\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 6/10\n",
            "5686/5686 [==============================] - 0s 19us/step - loss: 0.2331 - acc: 0.9223\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 7/10\n",
            "5686/5686 [==============================] - 0s 21us/step - loss: 0.2151 - acc: 0.9274\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 8/10\n",
            "5686/5686 [==============================] - 0s 18us/step - loss: 0.1994 - acc: 0.9323\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 9/10\n",
            "5686/5686 [==============================] - 0s 22us/step - loss: 0.1791 - acc: 0.9377\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 10/10\n",
            "5686/5686 [==============================] - 0s 21us/step - loss: 0.1596 - acc: 0.9435\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 376,
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1a3250d400>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 376,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.where(y_pred>0.5,1,0) #Convert to binary\n",
        "y_true = y_test\n",
        "Acc = accuracy_score(y_true, y_pred)\n",
        "F1 = f1_score(y_true, y_pred)\n",
        "print(\"Accuracy: \", Acc)\n",
        "print(\"F1 Score: \", F1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9392945036915504\n",
            "F1 Score:  0.9356521739130435\n"
          ]
        }
      ],
      "execution_count": 377,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesting to note here that with a simple \"Sequential\" net implemented in Keras, we can quickly achieve a ~93% accuracy and a ~93% F1 score. If we let the network train for more epochs, then we can easily push our performance even higher. However, our goal for this notebook is not to achieve the best model performance, but rather to develop an applied understanding of regularization - both practical and theoretical."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning - PyTorch\n",
        "\n",
        "------\n",
        "\n",
        "### SOURCES\n",
        "\n",
        "When learning how to construct feedforward networks (or any type of netowrk) in PyTorch, I referenced [Sung Kim's Github](https://github.com/hunkim). There, Sung Kim has layed out a repository called [PyTorch Zero To All](https://github.com/hunkim/PyTorchZeroToAll) in which he includes slide decks and code explaining how to build neural netowrks using pytorch from first principles. This was my main resource - in addition to Sung Kim's [Youtube Series](https://www.youtube.com/watch?v=SKq-pmkekTk) - which I used for the next section of this notebook. \n",
        "\n",
        "Another source which I have started to use when constructing networks in PyTorch is [a Stanford Github](https://cs230-stanford.github.io/pytorch-getting-started.html) account which outlines best practices for developing clean, well functioning netowrks in PyTorch. \n",
        "\n",
        "Other important sources include: \n",
        "\n",
        "\"Deep Learning\" by Ian Goodfellow and Yoshua Bengio and Aaron Courville. \n",
        "\n",
        "@book{Goodfellow-et-al-2016, <br>\n",
        "    title={Deep Learning}, <br>\n",
        "    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville}, <br>\n",
        "    publisher={MIT Press}, <br>\n",
        "    note={\\url{http://www.deeplearningbook.org}}, <br>\n",
        "    year={2016} <br>\n",
        "}"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define Network\n",
        "\n",
        "class Mushroom_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        \"super can be used to refer to parent classes without naming them explicitly\"\n",
        "        \n",
        "        We can now construct the network using torch.nn.linear layers \n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.l1 = torch.nn.Linear(22, 12)\n",
        "        #self.dropout = nn.Dropout(0.4) #<-- Overfitting could pose a problem due to small dataset size\n",
        "        self.l2 = torch.nn.Linear(12, 12)\n",
        "        #self.dropout = nn.Dropout(0.4) #<-- Overfitting could pose a problem due to small dataset size\n",
        "        self.l3 = torch.nn.Linear(12, 1)\n",
        "\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        The forward function takes in the input data and uses the sigmoid activation function to generate the output\n",
        "        \"\"\"\n",
        "        \n",
        "        out1 = self.sigmoid(self.l1(x))\n",
        "        out2 = self.sigmoid(self.l2(out1))\n",
        "        y_pred = self.sigmoid(self.l3(out2))\n",
        "        return y_pred"
      ],
      "outputs": [],
      "execution_count": 378,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Learning Parameters\n",
        "\n",
        "model = Mushroom_Net() #<-- Use the model created above \n",
        "#We use Binary Cross Entropy Loss as our loss function \n",
        "loss_fn = nn.BCELoss()\n",
        "#Adam is a very strong genric optimizer, so we can use it here\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "training_epochs = 50\n",
        "minibatch_size = 25"
      ],
      "outputs": [],
      "execution_count": 379,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t0 = time.clock() #define t0 so we can measure training time for different network variations\n",
        "\n",
        "for i in range(training_epochs):\n",
        "    if ((i%10) == 0):\n",
        "        print(\"Epoch\", i, \"/\", training_epochs) #Print our progress\n",
        "    \n",
        "    for batch_ind in range(4):\n",
        "        # wrap the data in variables\n",
        "        minibatch_state_var = Variable(torch.Tensor(X_train))\n",
        "        minibatch_label_var = Variable(torch.Tensor(y_train))\n",
        "        \n",
        "        # forward pass\n",
        "        y_pred = model(minibatch_state_var)\n",
        "        \n",
        "        # loss is MSE - defined above \n",
        "        # compute loss as the difference between the prediction and the true label \n",
        "        loss = loss_fn(y_pred, minibatch_label_var)\n",
        "\n",
        "        # now that the forward pass is done, we need to reset all gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # backwards pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # step the optimizer - update the weights\n",
        "        optimizer.step()\n",
        "        \n",
        "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \")     \n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"Number of Epochs Trained: \", training_epochs)\n",
        "print(\"With minibatch size: \", minibatch_size)\n",
        "print(\"Training time: \", time.clock()) \n",
        "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 / 50\n",
            "Epoch 10 / 50\n",
            "Epoch 20 / 50\n",
            "Epoch 30 / 50\n",
            "Epoch 40 / 50\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "TRAINING COMPLETE\n",
            "Number of Epochs Trained:  50\n",
            "With minibatch size:  25\n",
            "Training time:  335.781649\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([5686])) that is different to the input size (torch.Size([5686, 1])) is deprecated. Please ensure they have the same size.\n",
            "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
          ]
        }
      ],
      "execution_count": 380,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation\n",
        "\nWe can now use Accuracy and F1 Score to evaluate how our PyTorch model performed. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets see how we did! \n",
        "#Test the trained model with our holdout data \"X_test\"\n",
        "input_1 = Variable(torch.Tensor(X_test)) # \"variable\" simply creates a new tenosr\n",
        "input_1\n",
        "\n",
        "#Make predictions on the inputs \n",
        "pred_1 = model(input_1)\n",
        "\n",
        "pred_1_np = pred_1.detach().numpy()\n",
        "y_pred = np.where(pred_1_np>0.5,1,0) #Convert to binary\n",
        "y_true = y_test\n",
        "A = accuracy_score(y_true, y_pred)\n",
        "F1 = f1_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", A)\n",
        "print(\"F1 Score:\", F1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8556193601312552\n",
            "F1 Score: 0.8376383763837637\n"
          ]
        }
      ],
      "execution_count": 381,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, similar to the model generated in Keras using the TF library, we were able to quickly achieve pretty good model performance. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization\n",
        "\n",
        "------\n",
        "\n",
        "There is no generally accepted way to introduce specific regularization techniques to boost model performance or limit the negative effects of overfitting other than simple experementation and testing. James Bergstra and Yoshua Bengio outline this well in their 2012 paper, [Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) in which they discuess optimization of several hyperpapameters used in deep learning such as network topology (number of layers and neurons per layer) and learning rate. They conclude it's best to use a random search method as opposed to grid search or manual search when finding the correct hyperparameters which will yeild the best results from a network. \n",
        "\n",
        "There are many parameters in deep learning which can be optimized through use of regularization: the number of layers in the network, the numbner of neurons in each layer, the activation, the penalty or loss, the learning rate, the momentum value, etc. All of these values change the way that the network learns from the data, so by varying them we can change the model used to approximate the target function. \n",
        "\n",
        "Becasue we know that random search for hyperparameter optimization works better than grid search, we can create a certain range $R1$ and choose a certain number $n$ of samples which will be chosen from a random uniform distrobution between the upper and lower bounds of $R$. Similarly, we will have another hyperparameter chosen within a certain range $P1$ with its own upper and lower bounds. In the case below, let's choose $n = 10$. \n",
        "\nNote that all elements of $R1$ and $P1$ are real numbers. "
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_start = 10\n",
        "epoch_end = 100\n",
        "minibatch_start = 10\n",
        "minibatch_end = 75\n",
        "\n",
        "#Number of samples in each distrobution \n",
        "n = 10\n",
        "\n",
        "#Generate 5 samples from each distrobution\n",
        "Rand_E = np.random.uniform(epoch_start,epoch_end,n)\n",
        "Rand_M = np.random.uniform(minibatch_start,minibatch_end,n)\n",
        "\n",
        "Rand_E = np.around(Rand_E)\n",
        "Rand_M = np.around(Rand_M)\n",
        "\n",
        "#convert from float.64 to integer\n",
        "Rand_E = Rand_E.astype(int)\n",
        "Rand_M = Rand_M.astype(int)\n"
      ],
      "outputs": [],
      "execution_count": 388,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = range(1)\n",
        "y = range(1)\n",
        "fig = plt.figure(figsize=(5, 4))\n",
        "ax1 = fig.add_subplot(111)\n",
        "\n",
        "plt.scatter(Rand_E, Rand_M, color='blue', s=100, label=('{(r(i), p(i))}')) \n",
        "plt.legend(loc='upper left', fontsize=12);\n",
        "plt.xlabel('False Positive Rate', fontsize=20)\n",
        "plt.ylabel('True Positive Rate', fontsize=20)\n",
        "plt.title('Random Search for n=10 Trials', fontsize=16)\n",
        "plt.grid()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ],
            "image/png": [
              "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\n"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 389,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have our randomly distributed values for our first two hyperparameters, so we can use our chosen values $\\lambda1 \\epsilon[r(i), r(i+1), ... r(i+10)]$. Similarly, for the other hyperparameter being randomly varied and tested, we have $\\lambda2 \\epsilon[p(i), p(i+1), ... p(i+10)]$. Each of the ten pairs $[(r(i), p(i)), (r(i+1), p(i+1)), ... (r(n), p(n))]$ will be used to train the network, and the resulting model will then be tested using several metrics, and subsequently retrained. (See page 284 in [Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)). "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "n = Rand_E.shape[0] #Get length of hyperparameter distrobution vector \n",
        "\n",
        "accuracy_save = np.zeros(n)\n",
        "F1_save = np.zeros(n)\n",
        "\n",
        "for i in range(n): \n",
        "    print(\"------------------- n = \", i, \"-------------------\")\n",
        "    print(\"Epochs: \", Rand_E[i], \"with minibatch size: \", Rand_M[i])\n",
        "    \n",
        "    #Define Model\n",
        "    model = Mushroom_Net() #<-- Use the model created above \n",
        "    \n",
        "    #Declare new parameters \n",
        "    training_epochs = Rand_E[i]\n",
        "    minibatch_size = Rand_M[i]\n",
        "\n",
        "    #Train Network \n",
        "    for j in range(training_epochs):\n",
        "        for batch_ind in range(10):\n",
        "            # wrap the data in variables\n",
        "            minibatch_state_var = Variable(torch.Tensor(X_train))\n",
        "            minibatch_label_var = Variable(torch.Tensor(y_train))\n",
        "            # forward pass\n",
        "            y_pred = model(minibatch_state_var)\n",
        "            # compute loss as the difference between the prediction and the true label \n",
        "            loss = loss_fn(y_pred, minibatch_label_var)\n",
        "            # now that the forward pass is done, we need to reset all gradients\n",
        "            optimizer.zero_grad()\n",
        "            # backwards pass\n",
        "            loss.backward()\n",
        "            # step the optimizer - update the weights\n",
        "            optimizer.step()\n",
        "\n",
        "    \n",
        "    #Evaluate model performance\n",
        "    input_1 = Variable(torch.Tensor(X_test))\n",
        "    pred_1 = model(input_1)\n",
        "    pred_1_np = pred_1.detach().numpy()\n",
        "    y_pred = np.where(pred_1_np>0.5,1,0) #Convert to binary\n",
        "    y_true = y_test\n",
        "    A = accuracy_score(y_true, y_pred)\n",
        "    F1 = f1_score(y_true, y_pred)\n",
        "    \n",
        "    #Store evaluation information outside loop \n",
        "    accuracy_save[i] = A\n",
        "    F1_save[i] = F1\n",
        "  "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------- n =  0 -------------------\n",
            "Epochs:  39 with minibatch size:  47\n",
            "------------------- n =  1 -------------------\n",
            "Epochs:  35 with minibatch size:  71\n",
            "------------------- n =  2 -------------------\n",
            "Epochs:  32 with minibatch size:  58\n",
            "------------------- n =  3 -------------------\n",
            "Epochs:  20 with minibatch size:  16\n",
            "------------------- n =  4 -------------------\n",
            "Epochs:  13 with minibatch size:  23\n",
            "------------------- n =  5 -------------------\n",
            "Epochs:  86 with minibatch size:  56\n",
            "------------------- n =  6 -------------------\n",
            "Epochs:  50 with minibatch size:  65\n",
            "------------------- n =  7 -------------------\n",
            "Epochs:  48 with minibatch size:  37\n",
            "------------------- n =  8 -------------------\n",
            "Epochs:  94 with minibatch size:  36\n",
            "------------------- n =  9 -------------------\n",
            "Epochs:  46 with minibatch size:  35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([5686])) that is different to the input size (torch.Size([5686, 1])) is deprecated. Please ensure they have the same size.\n",
            "  # DeprecationWarning is ignored by default\n",
            "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ]
        }
      ],
      "execution_count": 399,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_save)\n",
        "print(F1_save)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.51558655 0.48441345 0.48441345 0.51558655 0.48441345 0.51558655\n",
            " 0.49589828 0.51558655 0.55086136 0.51558655]\n",
            "[0.         0.65266648 0.65266648 0.         0.65266648 0.\n",
            " 0.65525947 0.         0.66988242 0.        ]\n"
          ]
        }
      ],
      "execution_count": 400,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now find the point $(r(i), p(i))$ which maximizes both accuracy and F1 score. If training time was'nt a problem, we could also increase $n$ in order to make the random search more fine. \n",
        "\nThis process could not only be repreated with other parameters, but after finding the right \"zone\" of highest performance for a certain parameter, we could shrink the upper and lower bounds of the distrobution around that section and run the optomizer again. By repeating this process once or twice, we are making sure we've thouroughly explored the area which yeilds the best model output. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Regularization Techniques\n",
        "\n",
        "------\n",
        "\n",
        "There are many other regularization techniques used to boost model performance and limit the negative effects of overfitting. Some of the more popular techniques are: \n",
        "\n",
        "1. L1 Regularization\n",
        "2. L2 Regularization\n",
        "3. Dropout Layers\n",
        "4. Early Stopping\n",
        "\n",
        "## L1 Regularization \n",
        "Both L1 and L2 regularization are methods used to prevent overfitting your model to the dataset. There is only a small difference between the two: the penalty term in the regression. The L1 regularization technique uses \"Lasso Regression\" [Towards Data Science](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c) which \"adds absolute value of magnitude of coefficient as penalty term to the loss function.\" This simply means that instead of squaring the error in the loss function, we take the absolutie value of the difference. \n",
        "\n\n",
        "## L2 Regularization \n",
        "Similar to L1 regularization, L2 regularization is used to prevent overfitting the model to the data. The L2 model uses Ridge Regression, which \"adds squared magnitude of coefficient as penalty term to the loss function.\"[Towards Data Science](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c) This means that, as opposed to L1 regularization, here we square the difference to calculate the error in the loss function. \n",
        "\n",
        "## Dropout Layers \n",
        "Often used in conjunction with techniques such as L1 and L2 regularization, using dropout layers is a regularization technique used to prevent overfitting. Dropout layers force certain layers to \"drop\" neurons randomly with some set probabilit $p$ so that the network cannot rely on any certain set of neurons for success. Dropout layers force the network to continually re-learn from the training data; when the training is finished the the network will be used for prediction, the dropout layers are deactivated. \n",
        "\n",
        "## Early Stopping\n",
        "Early stopping can be used when too many epochs have been used to train a network, and the model is fitting the data too closely. Similar to the above regularization methods, early stopping can be used to mitigate the negative effects of overfitting. By plotting the training and testing accuracy and loss, we can determine when the testing loss stops decreasing, and actually brgins increasing. We take the minimum testing loss value, determine the epoch which corresponds to that value, and redefine and retrain the network using that many epochs. \n",
        "\n\n",
        "Sources: <br>\n",
        "[L1 and L2 Regularization Methods - Towards Data Science](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c)\n",
        "\n",
        "[Leonardo Araujo Santos - GitBooks.io](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/dropout_layer.html)\n",
        "\n",
        "[Machine Learning Mastery](https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/)\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Closing Remarks: AutoKeras\n",
        "\n",
        "-----\n",
        "\n",
        "Lastly, it's important to note that Google's AutoML has recently been recreated by Keras in an open source version of a similar software called [AutoKeras.](https://autokeras.com/) \n",
        "\n",
        "AutoKeras is a way of finding the best network parameters for the data system at hand without needing to do any parameter optimization using manual search, grid search, random search, or otherwise. \n",
        "\nAs we start to see more of these libraries released for free, it will be important to stay up to date on the best libraries which will provide a \"stress free\" parameter optimization and regularization experience. Still, it's always good to know the best practices for manual regularization both for benchmarking your own models and so that you can be sure libraries such as AutoKeras are performing better than simpler methods such as those discussed above. "
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}